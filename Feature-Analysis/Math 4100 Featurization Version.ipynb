{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction More Math 4100 Project \n",
    "Main difference: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Features  \n",
    "Features extracted and used for analysis/classification are below:\n",
    "* Total Number of Packets \n",
    "* Total Number of Bytes\n",
    "* Average inter-arrival time of packets\n",
    "* Average Byte-Rate of Connection (bytes per second)\n",
    "* Maximum packet size\n",
    "* Minimum packet size\n",
    "* Longest time between packet arrivals\n",
    "* Shortest time between packet arrivals\n",
    "* Direction of travel (Outgoing: 1, Incoming: 0)\n",
    "* Number of [ARP](https://erg.abdn.ac.uk/users/gorry/course/inet-pages/arp.html) packets \n",
    "* Number of [DNS](https://www.networkworld.com/article/3268449/what-is-dns-and-how-does-it-work.html)\n",
    "* Number of [TCP ACKs](http://packetlife.net/blog/2010/jun/7/understanding-tcp-sequence-acknowledgment-numbers/)\n",
    "* Maximum advertised [receive window](http://packetbomb.com/understanding-throughput-and-tcp-windows/)\n",
    "* Minimum advertised [receive window](http://packetbomb.com/understanding-throughput-and-tcp-windows/)  \n",
    "\n",
    "All are taken directly from [this paper](https://www.flux.utah.edu/paper/283) (Baker et al.).  \n",
    "\n",
    "Below is code that featurizes my data, returning the feature vectors for each file as a list of np arrays.  \n",
    "These arrays put into a dictionary of form: `data_set name : list of feature vectors` and saved as `.npy` files in the data directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSLimport numpy as np\n",
    "import netaddr\n",
    "import csv\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Given a CSV file of packet captures and the source and the ip source of capture, \n",
    "# extracts feature vectors for all packets in the file. \n",
    "# Separates packets into their connections and extracts a feature for every 2 seconds of a connection.\n",
    "#\n",
    "# Fetaure vectors are of length 14 \n",
    "#\n",
    "def featurize(filename, ip):\n",
    "    # tracks connection features (every 2-second window is featurized)\n",
    "    connections = {}\n",
    "    # list of feature vectors that is returned\n",
    "    return_data = []\n",
    "    start_time = 0.0\n",
    "    \n",
    "    # reading in the CSV file of packet captures\n",
    "    with open(filename, 'r') as data_file:\n",
    "        reader = csv.reader(data_file, delimiter=',')\n",
    "        # each packet is represented by each line\n",
    "        for index, line in enumerate(reader):\n",
    "            # line format: [0:frame number 1:Time 2:Source IP 3:Dest IP 4:Protocol 5:(Frame)Length 6:Info]\n",
    "            if line[1] != 'Time':\n",
    "                time = float(line[1])\n",
    "                protocol = line[4]\n",
    "                try:\n",
    "                    ip_src = int(netaddr.IPAddress(line[2]))\n",
    "                    ip_dst = int(netaddr.IPAddress(line[3]))\n",
    "                except:\n",
    "                    continue\n",
    "                # each unique connection has the key: ip_source-ip_dest-protocol, this defines a \"flow\"\n",
    "                key = str(ip_src) + '-' + str(ip_dst) + '-' + protocol\n",
    "                if key not in connections:                    \n",
    "                    # make new conncetion entry for new connections\n",
    "                    connections[key] = {'baseTime': start_time, 'lastArrival': time, \n",
    "                                        'endTime': start_time+2.0, 'longestInterArrival': 0, \n",
    "                                        'shortestInterArrival': 2, 'packets': 0, 'bytes': 0, \n",
    "                                        'largest_packet': 0, 'smallest_packet': 999999999999, \n",
    "                                        'direction': 0, 'arp_count': 0, 'dns_count': 0, 'ack_count': 0, \n",
    "                                        'min_cong_win': 999999999999, 'max_cong_win': 0}\n",
    "                    start_time += 2.0\n",
    "                conn_data = connections[key]\n",
    "                length = int(line[5])\n",
    "                extra_info = line[6]\n",
    "                # if current packet's time is after the 2 second window, create \n",
    "                # new feature vector and add previous one to returned feature vectors\n",
    "                if time > conn_data['endTime']:\n",
    "                    inter_arrival = ((conn_data['endTime'] - conn_data['baseTime']) / \n",
    "                                     conn_data['packets']) if conn_data['packets'] > 0 else 0\n",
    "                    bit_rate = conn_data['bytes'] / 2.0\n",
    "                    feature_vec = [conn_data['packets'], conn_data['bytes'], round(inter_arrival, 5), \n",
    "                                        round(bit_rate, 5), conn_data['largest_packet'], conn_data['smallest_packet'], \n",
    "                                        conn_data['longestInterArrival'], conn_data['shortestInterArrival'], \n",
    "                                        conn_data['direction'], conn_data['arp_count'], conn_data['dns_count'], \n",
    "                                        conn_data['ack_count'], conn_data['min_cong_win'], conn_data['max_cong_win']]\n",
    "                    return_data.append(np.array(feature_vec))\n",
    "                    connections[key] = {'baseTime': start_time, 'lastArrival': time, \n",
    "                                        'endTime': start_time+2.0, 'longestInterArrival': 0, \n",
    "                                        'shortestInterArrival': 2, 'packets': 0, 'bytes': 0, \n",
    "                                        'largest_packet': 0, 'smallest_packet': 999999999999, \n",
    "                                        'direction': 0, 'arp_count': 0, 'dns_count': 0, 'ack_count': 0, \n",
    "                                        'min_cong_win': 999999999999, 'max_cong_win': 0}\n",
    "                    conn_data = connections[key]\n",
    "                    start_time += 2.0\n",
    "                # updating connection statistics with the information in the current packet\n",
    "                if (time - conn_data['lastArrival']) > 0 and (time - conn_data['lastArrival']) < conn_data['shortestInterArrival']:\n",
    "                    conn_data['shortestInterArrival'] = time - conn_data['lastArrival']\n",
    "                if (time - conn_data['lastArrival']) > conn_data['longestInterArrival']:\n",
    "                    conn_data['longestInterArrival'] = time - conn_data['lastArrival']\n",
    "                if length > conn_data['largest_packet']:\n",
    "                    conn_data['largest_packet'] = length\n",
    "                if length < conn_data['smallest_packet']:\n",
    "                    conn_data['smallest_packet'] = length\n",
    "                # if packet's source is the capture ip then it's outgoing and direction feature is 1\n",
    "                if ip in line[2]:\n",
    "                    conn_data['direction'] = 1\n",
    "                if protocol == 'DNS':\n",
    "                    conn_data['dns_count'] += 1\n",
    "                if protocol.find('ARP') >= 0:\n",
    "                    conn_data['arp_count'] += 1\n",
    "                # extra info (last col of CSV) is used to get feaqture: Number of TCP ACKs \n",
    "                if protocol == 'TCP' and extra_info.find('[ACK]'):\n",
    "                    conn_data['ack_count'] += 1\n",
    "                # extra info (last col of CSV) is used to get features: Max/Min Advertised Receive Window\n",
    "                if protocol == 'TCP' and extra_info.find('Win=') >= 0:\n",
    "                    window_index = extra_info.index('Win=')\n",
    "                    if window_index >= 0:\n",
    "                        window_size = int(extra_info[window_index+4:extra_info.index(' ', window_index)])\n",
    "                        if window_size < conn_data['min_cong_win']:\n",
    "                            conn_data['min_cong_win'] = window_size\n",
    "                        if window_size > conn_data['max_cong_win']:\n",
    "                            conn_data['max_cong_win'] = window_size\n",
    "                conn_data['packets'] += 1\n",
    "                conn_data['bytes'] += length\n",
    "                conn_data['lastArrival'] = time\n",
    "    return return_data\n",
    "\n",
    "# Directory all data is stored in:\n",
    "data_dir = 'Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some notes on features:**  \n",
    "A feature vector is created for every 2 seconds of each connection or flow. A flow is defined by a TCP 5-Tuple that defines the connection through which packets are transmitted:  \n",
    "`{source port, sorce IP address, destination port, destination IP address, networking protocol}`  \n",
    "\n",
    "This means that even if a connection has no packets transmitted over a period of 2 seconds, a feature vector is still created. (Default feature vector values such as 'Number of Packets' = 0 are valid, and aren't discarded in dataset for analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirror labelled traffic dataset\n",
    "mirror1 = featurize(data_dir + '/mirror1.csv', '204.99.128.20')\n",
    "mirror2 = featurize(data_dir + '/mirror2.csv', '204.99.128.20')\n",
    "mirror3 = featurize(data_dir + '/mirror3.csv', '204.99.128.20')\n",
    "mirror4 = featurize(data_dir + '/mirror4.csv', '204.99.128.20')\n",
    "mirror5 = featurize(data_dir + '/mirror5.csv', '204.99.128.20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transfer Node labelled traffic dataset\n",
    "dtn1 = featurize(data_dir + '/dtn1.csv', '204.99.128.81')\n",
    "dtn2 = featurize(data_dir + '/dtn2.csv', '204.99.128.81')\n",
    "dtn3 = featurize(data_dir + '/dtn3.csv', '204.99.128.81')\n",
    "dtn4 = featurize(data_dir + '/dtn4.csv', '204.99.128.81')\n",
    "dtn5 = featurize(data_dir + '/dtn5.csv', '204.99.128.81')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube traffic labelled traffic dataset\n",
    "y1 = featurize(data_dir + '/youtube1.csv', '155.101.8.11')\n",
    "y2 = featurize(data_dir + '/youtube2.csv', '155.101.8.11')\n",
    "y3 = featurize(data_dir + '/youtube3.csv', '155.101.8.11')\n",
    "y4 = featurize(data_dir + '/youtube4.csv', '155.101.8.11')\n",
    "y5 = featurize(data_dir + '/youtube5.csv', '155.101.8.11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airplane traffic labelled dataset\n",
    "a1 = featurize(data_dir + '/airplane1.csv', '204.99.128.82')\n",
    "a2 = featurize(data_dir + '/airplane2.csv', '204.99.128.82')\n",
    "a3 = featurize(data_dir + '/airplane3.csv', '204.99.128.82')\n",
    "a4 = featurize(data_dir + '/airplane4.csv', '204.99.128.82')\n",
    "a5 = featurize(data_dir + '/airplane5.csv', '204.99.128.82')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mirror1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-efd02de240a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creating dictionary of all data, saving it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m data_dict = {'mirror1' : mirror1, 'mirror2' : mirror2, 'mirror3' : mirror3,\n\u001b[0m\u001b[1;32m      3\u001b[0m                 \u001b[0;34m'mirror4'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmirror4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mirror5'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmirror5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0;34m'dtn1'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdtn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtn2'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdtn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtn3'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdtn3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0;34m'dtn4'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdtn4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtn5'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdtn5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mirror1' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating dictionary of all data, saving it\n",
    "data_dict = {'mirror1' : mirror1, 'mirror2' : mirror2, 'mirror3' : mirror3,\n",
    "                'mirror4' : mirror4, 'mirror5' : mirror5,\n",
    "             'dtn1' : dtn1, 'dtn2' : dtn2, 'dtn3' : dtn3, \n",
    "                'dtn4' : dtn4, 'dtn5' : dtn5, \n",
    "             'youtube1' : y1, 'youtube2' : y2, 'youtube3' : y3,\n",
    "                 'youtube4' : y4, 'youtube5' : y5,\n",
    "             'airplane1' : a1, 'airplane2' : a2, 'airplane3' : a3,\n",
    "                 'airplane4' : a4, 'airplane5' : a5}\n",
    "\n",
    "np.save('Data/all_data.npy', data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
