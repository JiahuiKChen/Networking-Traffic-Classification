{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Feature Extraction\n",
    "  \n",
    "    \n",
    "These features are extracted per uni-directional flow:  \n",
    "\t0. Total Number of Packets\n",
    "\t1. Total Bytes\n",
    "\t2. Largest Packet Size\n",
    "\t3. Smallest Packet Size\n",
    "\t4. Number of ARP Packets\n",
    "\t5. Number of DNS Packets\n",
    "\t6. Number of TCP ACKs\n",
    "\t7. Minimum Advertised Receive Window\n",
    "\t8. Maximum Advertised Receive Window\n",
    "\t9. Direction (1 for outgoing, 0 for incoming)\n",
    "\t10. Std. Dev of packet size\n",
    "\t11. Average Packet Size\n",
    "\t12. Size of first 10 packets\n",
    "\t13. Number of TCP FIN: FIN in Info col. \n",
    "\t14. Number of TCP SYN: SYN in Info col.\n",
    "\t15. Number of TCP RSTS: RST in Info col.\n",
    "\t16. Number of TCP PUSH: PSH in Info col. \n",
    "\t17. Number of TCP URG: URG in Info col.\n",
    "\t18. Number of TCP CWR/CWE (Congestion Window Reduced)\n",
    "\t19. Number of TCP ECE (Explicit Congestion Notification Echo)\n",
    "\t20. Avg. Packet Inter-arrival time\n",
    "\t21. Max. Inter-arrival time\n",
    "\t22. Min. Inter-arrival time\n",
    "\t23. Avg. Packet Throughput (packets/second)\n",
    "\t24. Avg. Byte Throughput (bytes/second)\n",
    "    25: Duration\n",
    "    26: Standard Deviation of Packet Inter-Arrival Time\n",
    "  \n",
    "Note: Ryan's \"flows\" were not featurized, his sessions are. (IP Source and IP Dest were only things used to create features for, no ports, so multiple flows (by TCP 5 Tuple) were featurized together).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netaddr\n",
    "import csv\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "#\n",
    "# Extracts a feature vector for each flow in the input file.\n",
    "#\n",
    "# A flow is defined by a unique bi-directional tuple: \n",
    "# {IP Source Address, Source Port, IP Destination Address, Destination Port, Protocol}\n",
    "# \n",
    "# The 26 features above are extracted for each feature in the given datafile. \n",
    "# The src_ip parameter is used to determine the Number of Incoming and Outgoing Packets features.\n",
    "#\n",
    "def extract_flow_features(filename, src_ip):\n",
    "    # Dictionary holding values for feature calculations with keys of:\n",
    "    # 'ip_source-source_port-ip_dest-dest_port-protocol'\n",
    "    flows = {}\n",
    "    \n",
    "    # Returned feature values extracted from the given file\n",
    "    feature_vecs = []\n",
    "    \n",
    "    # Reading in the CSV file of packet captures\n",
    "    with open(filename, 'r') as data_csv:\n",
    "        reader = csv.reader(data_csv, delimiter=',')\n",
    "        # Loop through all lines (all packets), fill in statistics of each flow found\n",
    "        for index, line in enumerate(reader):\n",
    "            # Line/CSV row format (represents a packet): \n",
    "            # [0:Time (Packet Arrival) 1:Source IP 2:Dest IP 3:Protocol \n",
    "            #  4: Length 5:Info 6:Source Port 7: Dest Port]\n",
    "            if line[0] == 'Time':\n",
    "                continue\n",
    "            # Parse out fields needed to construct key (if not the first CSV line)\n",
    "            ip_src = line[1]\n",
    "            src_port = line[6]\n",
    "            ip_dest = line[2]\n",
    "            dest_port = line[7]\n",
    "            protocol = line[3]\n",
    "            time = float(line[0])\n",
    "            \n",
    "            key = ip_src + '-' + src_port + '-' + ip_dest + '-' + dest_port + '-' + protocol\n",
    "            \n",
    "            # Create new flow and flow statistics if a new flow is encountered\n",
    "            if key not in flows: \n",
    "                flows[key] = {'tot_bytes': 0, 'direction': 0, 'ARP': 0, 'DNS': 0,  \n",
    "                              'ACK': 0, 'min_arw': 1000000, 'max_arw': 0, 'FIN': 0,\n",
    "                              'SYN': 0, 'RST': 0, 'PUSH': 0, 'URG': 0, 'CWE': 0, \n",
    "                              'ECE': 0, 'start': time, 'end': 0,\n",
    "                              'all_sizes': [], 'all_intervals': [], 'last_arrival': None}\n",
    "                # Direction of flow is consistent throughout all packets\n",
    "                flows[key]['direction'] = 1 if ip_src == src_ip else 0\n",
    "            \n",
    "            # Update flow statistics based on current packet's information\n",
    "            flow_stats = flows[key]\n",
    "            length = float(line[4])\n",
    "            info = line[5]\n",
    "            \n",
    "            # Total bytes update\n",
    "            flow_stats['tot_bytes'] += length\n",
    "            # Number of ARP Packets update\n",
    "            if 'ARP' in protocol:\n",
    "                flow_stats['ARP'] += 1\n",
    "            # Number of DNS Packets update\n",
    "            if 'DNS' in protocol:\n",
    "                flow_stats['DNS'] += 1\n",
    "            # Number of ACK Packets update\n",
    "            if (protocol == 'TCP' and 'ACK' in info):\n",
    "                flow_stats['ACK'] += 1\n",
    "            # Minimum or Maximum Advertised Receive Window update\n",
    "            if protocol == 'TCP' and 'Win=' in info:\n",
    "                win_ind = info.index('Win=')\n",
    "                # Window size is immediately after the 'Win=' and goes until next space\n",
    "                win_size = int(info[win_ind+4 : info.index(' ', win_ind)])\n",
    "                if win_size < flow_stats['min_arw']:\n",
    "                    flow_stats['min_arw'] = win_size\n",
    "                elif win_size > flow_stats['max_arw']:\n",
    "                    flow_stats['max_arw'] = win_size\n",
    "            # Number of FIN Packets update\n",
    "            if protocol == 'TCP' and 'FIN' in info:\n",
    "                flow_stats['FIN'] += 1\n",
    "            # Number of SYN Packets update\n",
    "            if protocol == 'TCP' and 'SYN' in info:\n",
    "                flow_stats['SYN'] += 1\n",
    "            # Number of RST Packets update\n",
    "            if protocol == 'TCP' and 'RST' in info:\n",
    "                flow_stats['RST'] += 1\n",
    "            # Number of PUSH Packets update\n",
    "            if protocol == 'TCP' and 'PUSH' in info:\n",
    "                flow_stats['PUSH'] += 1\n",
    "            # Number of URG Packets update\n",
    "            if protocol == 'TCP' and 'URG' in info:\n",
    "                flow_stats['URG'] += 1\n",
    "            # Number of CWE Packets update\n",
    "            if protocol == 'TCP' and ('CWE' in info or 'CWR' in info):\n",
    "                flow_stats['CWE'] += 1\n",
    "            # Number of ECE Packets update\n",
    "            if protocol == 'TCP' and 'ECE' in info:\n",
    "                flow_stats['ECE'] += 1\n",
    "            # End time update (every packet that's not first becomes most recent end)\n",
    "            flow_stats['end'] = time\n",
    "            # Adding this packet's size to array of all packet sizes\n",
    "            flow_stats['all_sizes'].append(length)\n",
    "            # Adding interval between this packet's arrival and last packet's arrival \n",
    "            # to array of all inter-packet arrival times, but don't take the first packet's time\n",
    "            if flow_stats['last_arrival'] is not None:\n",
    "                flow_stats['all_intervals'].append(time - flow_stats['last_arrival'])\n",
    "            # Update last arrival time\n",
    "            flow_stats['last_arrival'] = time\n",
    "    \n",
    "    # After all packets are processed, calculate feature values for each flow    \n",
    "    for flow_key in flows.keys():\n",
    "        stats = flows[flow_key]\n",
    "        all_pckt_sizes = np.array(stats['all_sizes'])  \n",
    "        # only 1 packet: (H1/CERN protocol packets, etc.) then all features involving duraiton are 0\n",
    "        if all_pckt_sizes.size == 1:\n",
    "            flow_features = np.array([all_pckt_sizes.size, stats['tot_bytes'], np.max(all_pckt_sizes),\n",
    "                                  np.min(all_pckt_sizes), stats['ARP'], stats['DNS'], stats['ACK'],\n",
    "                                  stats['min_arw'], stats['max_arw'], stats['direction'], \n",
    "                                  np.std(all_pckt_sizes), np.average(all_pckt_sizes), np.sum(all_pckt_sizes),\n",
    "                                  stats['FIN'], stats['SYN'], stats['RST'], stats['PUSH'],\n",
    "                                  stats['URG'], stats['CWE'], stats['ECE'], 0,\n",
    "                                  0, 0, 0, 0, 0, 0])\n",
    "        else:\n",
    "            all_pckt_intervals = np.array(stats['all_intervals'])\n",
    "            first_sizes = np.sum(all_pckt_sizes[:10]) if all_pckt_sizes.size >= 10 else np.sum(all_pckt_sizes)\n",
    "            duration = stats['end'] - stats['start'] #if all_pckt_sizes.size > 1 else 0\n",
    "            byte_rate = stats['tot_bytes']/duration #if duration > 0 else 0\n",
    "            pckt_rate = all_pckt_sizes.size/duration #if duration > 0 else 0\n",
    "            flow_features = np.array([all_pckt_sizes.size, stats['tot_bytes'], np.max(all_pckt_sizes),\n",
    "                                  np.min(all_pckt_sizes), stats['ARP'], stats['DNS'], stats['ACK'],\n",
    "                                  stats['min_arw'], stats['max_arw'], stats['direction'], \n",
    "                                  np.std(all_pckt_sizes), np.average(all_pckt_sizes), first_sizes,\n",
    "                                  stats['FIN'], stats['SYN'], stats['RST'], stats['PUSH'],\n",
    "                                  stats['URG'], stats['CWE'], stats['ECE'], np.average(all_pckt_intervals),\n",
    "                                  np.max(all_pckt_intervals), np.min(all_pckt_intervals), \n",
    "                                  pckt_rate, byte_rate, duration, np.std(all_pckt_intervals)]) #np.std(all_pckt_intervals)\n",
    "        feature_vecs.append(flow_features)\n",
    "    \n",
    "    # Return a list of feature vectors, one per flow \n",
    "    # ALSO RETURNS DICTIONARY OF FLOW STATISTICS FOR TESTING\n",
    "    return np.array(feature_vecs), flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../DT-Data/\"\n",
    "dtn1_ip = '204.99.128.105'\n",
    "clustereddtn_ip = '204.99.128.81'\n",
    "kchow_ip = '155.101.8.11'\n",
    "airplane2_ip = '204.99.128.82'\n",
    "gdrive_ip = '172.217.11.170'\n",
    "gdrive_ip2 = '172.217.4.138'\n",
    "gdrive_ip3 = '172.217.5.74'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flow feature vectors from globus-dtn1-dest-iso: 16\n",
      "total size:  9300144.0\n",
      "first 10:  364116.0\n",
      "total size:  8795910.0\n",
      "first 10:  186012.0\n",
      "total size:  8162804.0\n",
      "first 10:  51348.0\n",
      "total size:  8076976.0\n",
      "first 10:  184564.0\n",
      "total size:  9168728.0\n",
      "first 10:  668.0\n",
      "total size:  11324156.0\n",
      "first 10:  668.0\n",
      "total size:  9003266.0\n",
      "first 10:  668.0\n",
      "total size:  8745866.0\n",
      "first 10:  668.0\n",
      "total size:  411478797.0\n",
      "first 10:  55772.0\n",
      "total size:  499307193.0\n",
      "first 10:  57220.0\n",
      "total size:  404358194.0\n",
      "first 10:  61564.0\n",
      "total size:  396705164.0\n",
      "first 10:  61564.0\n",
      "total size:  2372.0\n",
      "first 10:  2372.0\n",
      "total size:  2372.0\n",
      "first 10:  2372.0\n",
      "total size:  2372.0\n",
      "first 10:  2372.0\n",
      "total size:  2372.0\n",
      "first 10:  2372.0\n"
     ]
    }
   ],
   "source": [
    "# # Globus Flows feature extractions\n",
    "# globus_dtn1_src1 = data_dir + 'globus-dtn1-src-iso.csv'\n",
    "# globus_dtn1_src1, test_flows1 = extract_flow_features(globus_dtn1_src1, dtn1_ip)\n",
    "# print(f'Number of flow feature vectors from globus-dtn1-src-iso: {len(globus_dtn1_src1)}')\n",
    "\n",
    "# globus_dtn1_dest1 = data_dir + 'globus-dtn1-dest-iso.csv'\n",
    "# globus_dtn1_dest1, test_flows = extract_flow_features(globus_dtn1_dest1, kchow_ip)\n",
    "# print(f'Number of flow feature vectors from globus-dtn1-dest-iso: {len(globus_dtn1_dest1)}')\n",
    "\n",
    "# globus_dtn1_src2 = data_dir + 'globus-dtn1-src-iso2.csv'\n",
    "# globus_dtn1_src2, test_flows = extract_flow_features(globus_dtn1_src2, dtn1_ip)\n",
    "# print(f'Number of flow feature vectors from globus-dtn1-src-iso2.csv: {len(globus_dtn1_src2)}')\n",
    "\n",
    "# globus_dtn1_dest2 = data_dir + 'globus-dtn1-dest-iso2.csv'\n",
    "# globus_dtn1_dest2, test_flows = extract_flow_features(globus_dtn1_dest2, kchow_ip)\n",
    "# print(f'Number of flow feature vectors from globus-dtn1-dest-iso: {len(globus_dtn1_dest2)}')\n",
    "\n",
    "# globus_clusterdtn_src = data_dir + 'globus-clusterdtn-src-iso.csv'\n",
    "# globus_clusterdtn_src, test_flows = extract_flow_features(globus_clusterdtn_src, clustereddtn_ip)\n",
    "# print(f'Number of flow feature vectors from globus-dtn1-src-iso2.csv: {len(globus_clusterdtn_src)}')\n",
    "\n",
    "globus_clusterdtn_dest = data_dir + 'globus-clusterdtn-dest-iso.csv'\n",
    "globus_clusterdtn_dest, test_flows = extract_flow_features(globus_clusterdtn_dest, kchow_ip)\n",
    "print(f'Number of flow feature vectors from globus-dtn1-dest-iso: {len(globus_clusterdtn_dest)}')\n",
    "\n",
    "# Verify the duration calculation\n",
    "# for i, flow in enumerate(test_flows.keys()):\n",
    "#         print('start: ', test_flows[flow]['start'], '   end: ', test_flows[flow]['end'])\n",
    "#         print(globus_clusterdtn_dest[i][25])\n",
    "\n",
    "# Checking TCP FIN and SYN packet counts\n",
    "# for i, flow in enumerate(test_flows.keys()):\n",
    "#     print(flow)\n",
    "#     print(globus_clusterdtn_dest[i])\n",
    "#     print('TCP FIN: ', globus_clusterdtn_dest[i][13])\n",
    "#     print('TCP SYN: ', globus_clusterdtn_dest[i][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flow feature vectors from fdt-airplane2-dest-iso-1stream: 3\n",
      "Total number of packets:  26660.0\n",
      "ARP Packets:  0.0\n",
      "TCP ACKs:  26660.0\n",
      "Total number of packets:  581225.0\n",
      "ARP Packets:  0.0\n",
      "TCP ACKs:  581225.0\n",
      "Total number of packets:  1.0\n",
      "ARP Packets:  0.0\n",
      "TCP ACKs:  0.0\n"
     ]
    }
   ],
   "source": [
    "# # FDT Flows feature extractions\n",
    "# fdt_a2_src = data_dir + 'fdt-airplane2-src-iso.csv'\n",
    "# fdt_a2_src, test_flows = extract_flow_features(fdt_a2_src, airplane2_ip)\n",
    "# print(f'Number of flow feature vectors from fdt-airplane2-src-iso: {len(fdt_a2_src)}')\n",
    "\n",
    "# fdt_a2_dest = data_dir + 'fdt-airplane2-dest-iso.csv'\n",
    "# fdt_a2_dest, test_flows = extract_flow_features(fdt_a2_dest, kchow_ip)\n",
    "# print(f'Number of flow feature vectors from fdt-airplane2-dest-iso: {len(fdt_a2_dest)}')\n",
    "\n",
    "# 1 stream configured transfer\n",
    "fdt_a2_dest_1str = data_dir + 'fdt-airplane2-dest-iso-1stream.csv'\n",
    "fdt_a2_dest_1str, test_flows = extract_flow_features(fdt_a2_dest_1str, kchow_ip)\n",
    "print(f'Number of flow feature vectors from fdt-airplane2-dest-iso-1stream: {len(fdt_a2_dest_1str)}')\n",
    "\n",
    "# 2 stream configured transfer\n",
    "# fdt_a2_dest_2str = data_dir + 'fdt-airplane2-dest-iso-2stream.csv'\n",
    "# fdt_a2_dest_2str, test_flows = extract_flow_features(fdt_a2_dest_2str, kchow_ip)\n",
    "# print(f'Number of flow feature vectors from fdt-airplane2-dest-iso-2stream: {len(fdt_a2_dest_2str)}')\n",
    "\n",
    "# fdt_dtn1_dest = data_dir + 'fdt-dtn1-dest-iso.csv'\n",
    "# fdt_dtn1_dest, test_flows = extract_flow_features(fdt_dtn1_dest, kchow_ip)\n",
    "# print(f'Number of flow feature vectors from fdt-dtn1-dest-iso: {len(fdt_dtn1_dest)}')\n",
    "\n",
    "# fdt_dtn1_src = data_dir + 'fdt-dtn1-src-iso.csv'\n",
    "# fdt_dtn1_src, test_flows = extract_flow_features(fdt_dtn1_src, dtn1_ip)\n",
    "# print(f'Number of flow feature vectors from fdt-dtn1-src-iso: {len(fdt_dtn1_src)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flow feature vectors from rclone-gdrive-src-iso: 5\n",
      "Number of flow feature vectors from rclone-gdrive-src-iso: 4\n",
      "Number of flow feature vectors from rclone-gdrive-src-iso2: 4\n",
      "Number of flow feature vectors from rclone-gdrive-dest-iso2: 4\n",
      "Number of flow feature vectors from rclone-gdrive-src-iso2: 4\n",
      "Number of flow feature vectors from rclone-gdrive-dest-iso3: 4\n"
     ]
    }
   ],
   "source": [
    "# RClone Flows feature extractions\n",
    "rclone_src = data_dir + 'rclone-gdrive-src-iso.csv'\n",
    "rclone_src, test_flows1 = extract_flow_features(rclone_src, gdrive_ip)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-src-iso: {len(rclone_src)}')\n",
    "\n",
    "rclone_dest = data_dir + 'rclone-gdrive-dest-iso.csv'\n",
    "rclone_dest, test_flows = extract_flow_features(rclone_dest, kchow_ip)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-src-iso: {len(rclone_dest)}')\n",
    "    \n",
    "rclone_src2 = data_dir + 'rclone-gdrive-src-iso2.csv'\n",
    "rclone_src2, test_flows2 = extract_flow_features(rclone_src2, gdrive_ip2)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-src-iso2: {len(rclone_src2)}')\n",
    "\n",
    "rclone_dest2 = data_dir + 'rclone-gdrive-dest-iso2.csv'\n",
    "rclone_dest2, test_flows = extract_flow_features(rclone_dest2, kchow_ip)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-dest-iso2: {len(rclone_dest2)}')\n",
    "\n",
    "rclone_src3 = data_dir + 'rclone-gdrive-src-iso3.csv'\n",
    "rclone_src3, test_flows3 = extract_flow_features(rclone_src3, gdrive_ip3)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-src-iso2: {len(rclone_src3)}')\n",
    "\n",
    "rclone_dest3 = data_dir + 'rclone-gdrive-dest-iso3.csv'\n",
    "rclone_dest3, test_flows = extract_flow_features(rclone_dest3, kchow_ip)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-dest-iso3: {len(rclone_dest3)}')\n",
    "\n",
    "# Checking packet inter-arrival time stuff\n",
    "# for i, flow in enumerate(test_flows1.keys()):\n",
    "#     print(flow)\n",
    "#     print(rclone_src[i])\n",
    "#     print('Average packet inter-arrival time: ', rclone_src[i][20])\n",
    "#     print('Average max inter-arrival time: ', rclone_src[i][21])\n",
    "\n",
    "# # Printing flows extracted\n",
    "# for key in test_flows.keys():\n",
    "#     print(key)\n",
    "    \n",
    "# # Sanity check on direction feature\n",
    "# for key in test_flows.keys():\n",
    "#     direction = test_flows[key]['direction']\n",
    "#     print(direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of all feature vectors mapping by capture file, saving it\n",
    "data_dict = {'globus_dtn1_src1' : globus_dtn1_src1, 'globus_dtn1_dest1' : globus_dtn1_dest1, \n",
    "             'globus_dtn1_src2' : globus_dtn1_src2, 'globus_dtn1_dest2' : globus_dtn1_dest2,\n",
    "             'globus_clusterdtn_src' : globus_clusterdtn_src, 'globus_clusterdtn_dest' : globus_clusterdtn_dest,\n",
    "             'fdt_a2_src' : fdt_a2_src, 'fdt_a2_dest' : fdt_a2_src, 'fdt_dtn1_dest' : fdt_dtn1_dest, \n",
    "             'fdt_dtn1_src' : fdt_dtn1_src, 'fdt_a2_dest_1str' : fdt_a2_dest_1str, 'fdt_a2_dest_2str' : fdt_a2_dest_2str,\n",
    "             'rclone_src' : rclone_src, 'rclone_dest' : rclone_dest, 'rclone_src2': rclone_src2, \n",
    "             'rclone_dest2': rclone_dest2, 'rclone_src3' : rclone_src3, 'rclone_dest3' : rclone_dest3}\n",
    "\n",
    "np.save('../Feature-Vectors/flow_features.npy', data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
