{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Feature Extraction\n",
    "## TODO: MAKE CHANGES ##\n",
    "    \n",
    "These features are extracted per bi-directional flow:  \n",
    "\t0. Total Number of Packets\n",
    "\t1. Total Bytes\n",
    "\t2. Largest Packet Size\n",
    "\t3. Smallest Packet Size\n",
    "\t4. Number of ARP Packets\n",
    "\t5. Number of DNS Packets\n",
    "\t6. Number of TCP ACKs\n",
    "\t7. Minimum Advertised Receive Window\n",
    "\t8. Maximum Advertised Receive Window\n",
    "\t9. Direction of Outgoing Packets\n",
    "\t10. Std. Dev of packet size\n",
    "\t11. Average Packet Size\n",
    "\t12. Size of first 10 packets\n",
    "\t13. Number of TCP FIN: FIN in Info col. \n",
    "\t14. Number of TCP SYN: SYN in Info col.\n",
    "\t15. Number of TCP RSTS: RST in Info col.\n",
    "\t16. Number of TCP PUSH: PSH in Info col. \n",
    "\t17. Number of TCP URG: URG in Info col.\n",
    "\t18. Number of TCP CWR/CWE (Congestion Window Reduced)\n",
    "\t19. Number of TCP ECE (Explicit Congestion Notification Echo)\n",
    "\t20. Avg. Packet Inter-arrival time\n",
    "\t21. Max. Inter-arrival time\n",
    "\t22. Min. Inter-arrival time\n",
    "\t23. Avg. Packet Throughput (packets/second)\n",
    "\t24. Avg. Byte Throughput (bytes/second)\n",
    "    25: Duration\n",
    "  \n",
    "Note: Ryan's \"flows\" were not featurized, his sessions are. (IP Source and IP Dest were only things used to create features for, no ports, so multiple flows (by TCP 5 Tuple) were featurized together).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netaddr\n",
    "import csv\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "#\n",
    "# Extracts a feature vector for each flow in the input file.\n",
    "#\n",
    "# A flow is defined by a unique bi-directional tuple: \n",
    "# {IP Source Address, Source Port, IP Destination Address, Destination Port, Protocol}\n",
    "# \n",
    "# The 26 features above are extracted for each feature in the given datafile. \n",
    "# The src_ip parameter is used to determine the Number of Incoming and Outgoing Packets features.\n",
    "#\n",
    "def extract_flow_features(filename, src_ip):\n",
    "    # Dictionary holding values for feature calculations with keys of:\n",
    "    # 'ip_source-source_port-ip_dest-dest_port-protocol'\n",
    "    flows = {}\n",
    "    \n",
    "    # Returned feature values extracted from the given file\n",
    "    feature_vecs = []\n",
    "    \n",
    "    # Reading in the CSV file of packet captures\n",
    "    with open(filename, 'r') as data_csv:\n",
    "        reader = csv.reader(data_csv, delimiter=',')\n",
    "        # Loop through all lines (all packets), fill in statistics of each flow found\n",
    "        for index, line in enumerate(reader):\n",
    "            # Line/CSV row format (represents a packet): \n",
    "            # [0:Time (Packet Arrival) 1:Source IP 2:Dest IP 3:Protocol \n",
    "            #  4: Length 5:Info 6:Source Port 7: Dest Port]\n",
    "            if line[0] == 'Time':\n",
    "                continue\n",
    "            # Parse out fields needed to construct key (if not the first CSV line)\n",
    "            ip_src = line[1]\n",
    "            src_port = line[6]\n",
    "            ip_dest = line[2]\n",
    "            dest_port = line[7]\n",
    "            protocol = line[3]\n",
    "            time = float(line[0])\n",
    "            \n",
    "            key = ip_src + '-' + src_port + '-' + ip_dest + '-' + dest_port + '-' + protocol\n",
    "            \n",
    "            # Create new flow and flow statistics if a new flow is encountered\n",
    "            if key not in flows: \n",
    "                flows[key] = {'tot_bytes': 0, 'direction': 0, 'ARP': 0, 'DNS': 0,  \n",
    "                              'ACK': 0, 'min_arw': 1000000, 'max_arw': 0, 'FIN': 0,\n",
    "                              'SYN': 0, 'RST': 0, 'PUSH': 0, 'URG': 0, 'CWE': 0, \n",
    "                              'ECE': 0, 'start': time, 'end': 0,\n",
    "                              'all_sizes': [], 'all_intervals': [], 'last_arrival': 0}\n",
    "                # Direction of flow is consistent throughout all packets\n",
    "                flows[key]['direction'] = 1 if ip_src == src_ip else 0\n",
    "            \n",
    "            # Update flow statistics based on current packet's information\n",
    "            flow_stats = flows[key]\n",
    "            length = float(line[4])\n",
    "            info = line[5]\n",
    "            \n",
    "            # Total bytes update\n",
    "            flow_stats['tot_bytes'] += length\n",
    "            # THIS UPDATE WOULD BE FOR BI-DIRECTIONAL FLOWS, W COUNTS OF PACKETS \n",
    "            # Number of outgoing packets update\n",
    "#             if ip_src == src_ip:\n",
    "#                 flow_stats['num_outgoing'] += 1\n",
    "            # Number of ARP Packets update\n",
    "            if 'ARP' in protocol:\n",
    "                flow_stats['ARP'] += 1\n",
    "            # Number of DNS Packets update\n",
    "            if 'DNS' in protocol:\n",
    "                flow_stats['DNS'] += 1\n",
    "            # Number of ACK Packets update\n",
    "            if protocol == 'TCP' and 'ACK' in info:\n",
    "                flow_stats['ACK'] += 1\n",
    "            # Minimum or Maximum Advertised Receive Window update\n",
    "            if protocol == 'TCP' and 'Win=' in info:\n",
    "                win_ind = info.index('Win=')\n",
    "                # Window size is immediately after the 'Win=' and goes until next space\n",
    "                win_size = int(info[win_ind+4 : info.index(' ', win_ind)])\n",
    "                if win_size < flow_stats['min_arw']:\n",
    "                    flow_stats['min_arw'] = win_size\n",
    "                elif win_size > flow_stats['max_arw']:\n",
    "                    flow_stats['max_arw'] = win_size\n",
    "            # Number of FIN Packets update\n",
    "            if protocol == 'TCP' and 'FIN' in info:\n",
    "                flow_stats['FIN'] += 1\n",
    "            # Number of SYN Packets update\n",
    "            if protocol == 'TCP' and 'SYN' in info:\n",
    "                flow_stats['SYN'] += 1\n",
    "            # Number of RST Packets update\n",
    "            if protocol == 'TCP' and 'RST' in info:\n",
    "                flow_stats['RST'] += 1\n",
    "            # Number of PUSH Packets update\n",
    "            if protocol == 'TCP' and 'PUSH' in info:\n",
    "                flow_stats['PUSH'] += 1\n",
    "            # Number of URG Packets update\n",
    "            if protocol == 'TCP' and 'URG' in info:\n",
    "                flow_stats['URG'] += 1\n",
    "            # Number of CWE Packets update\n",
    "            if protocol == 'TCP' and ('CWE' in info or 'CWR' in info):\n",
    "                flow_stats['CWE'] += 1\n",
    "            # Number of ECE Packets update\n",
    "            if protocol == 'TCP' and 'ECE' in info:\n",
    "                flow_stats['ECE'] += 1\n",
    "            # End time update (every packet that's not first becomes most recent end)\n",
    "            flow_stats['end'] = time\n",
    "            # Adding this packet's size to array of all packet sizes\n",
    "            flow_stats['all_sizes'].append(length)\n",
    "            # Adding interval between this packet's arrival and last packet's arrival \n",
    "            # to array of all inter-packet arrival times\n",
    "            flow_stats['all_intervals'].append(time - flow_stats['last_arrival'])\n",
    "            # Update last arrival time\n",
    "            flow_stats['last_arrival'] = time\n",
    "    \n",
    "    # After all packets are processed, calculate feature values for each flow\n",
    "    \n",
    "    # TODO: HANDLE 1 PACKET SITUATION FOR DURATION/RATES\n",
    "    \n",
    "    for flow_key in flows.keys():\n",
    "        stats = flows[flow_key]\n",
    "        all_pckt_sizes = np.array(stats['all_sizes'])    \n",
    "        all_pckt_intervals = np.array(stats['all_intervals'])\n",
    "        first_sizes = np.sum(all_pckt_sizes[:10]) if all_pckt_sizes.size >= 10 else np.sum(all_pckt_sizes)\n",
    "        duration = stats['end'] - stats['start'] if all_pckt_sizes.size >= 1 else 0\n",
    "        byte_rate = stats['tot_bytes']/duration\n",
    "        pckt_rate = all_pckt_sizes.size / duration\n",
    "        flow_features = np.array([len(all_pckt_sizes), stats['tot_bytes'], np.max(all_pckt_sizes),\n",
    "                                  np.min(all_pckt_sizes), stats['ACK'], stats['DNS'], stats['ACK'],\n",
    "                                  stats['min_arw'], stats['max_arw'], stats['direction'], \n",
    "                                  np.std(all_pckt_sizes), np.average(all_pckt_sizes), first_sizes,\n",
    "                                  stats['FIN'], stats['SYN'], stats['RST'], stats['PUSH'],\n",
    "                                  stats['URG'], stats['CWE'], stats['ECE'], np.average(all_pckt_intervals),\n",
    "                                  np.max(all_pckt_intervals), np.min(all_pckt_intervals), \n",
    "                                  pckt_rate, byte_rate, duration]) #np.std(all_pckt_intervals)\n",
    "        feature_vecs.append(flow_features)\n",
    "    \n",
    "    # Return a list of feature vectors, one per flow \n",
    "    # ALSO RETURNS DICTIONARY OF FLOW STATISTICS FOR TESTING\n",
    "    return np.array(feature_vecs), flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../DT-Data/\"\n",
    "dtn1_ip = '204.99.128.105'\n",
    "clustereddtn_ip = '204.99.128.81'\n",
    "kchow_ip = '155.101.8.11'\n",
    "airplane2_ip = '204.99.128.82'\n",
    "gdrive_ip = '172.217.11.170'\n",
    "gdrive_ip2 = '172.217.4.138'\n",
    "gdrive_ip3 = '172.217.5.74'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flow feature vectors from globus-dtn1-src-iso: 5\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-093e8674207c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mglobus_dtn1_dest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'globus-dtn1-dest-iso.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mglobus_dtn1_dest1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_flows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_flow_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobus_dtn1_dest1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkchow_ip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Number of flow feature vectors from globus-dtn1-dest-iso: {len(globus_dtn1_dest1)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-79f10efb366f>\u001b[0m in \u001b[0;36mextract_flow_features\u001b[0;34m(filename, src_ip)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mfirst_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pckt_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mall_pckt_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pckt_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mall_pckt_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mbyte_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tot_bytes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mpckt_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_pckt_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         flow_features = np.array([len(all_pckt_sizes), stats['tot_bytes'], np.max(all_pckt_sizes),\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# Globus Flows feature extractions\n",
    "globus_dtn1_src1 = data_dir + 'globus-dtn1-src-iso.csv'\n",
    "globus_dtn1_src1, test_flows = extract_flow_features(globus_dtn1_src1, dtn1_ip)\n",
    "print(f'Number of flow feature vectors from globus-dtn1-src-iso: {len(globus_dtn1_src1)}')\n",
    "\n",
    "globus_dtn1_dest1 = data_dir + 'globus-dtn1-dest-iso.csv'\n",
    "globus_dtn1_dest1, test_flows = extract_flow_features(globus_dtn1_dest1, kchow_ip)\n",
    "print(f'Number of flow feature vectors from globus-dtn1-dest-iso: {len(globus_dtn1_dest1)}')\n",
    "\n",
    "globus_dtn1_src2 = data_dir + 'globus-dtn1-src-iso2.csv'\n",
    "globus_dtn1_src2, test_flows = extract_flow_features(globus_dtn1_src2, dtn1_ip)\n",
    "print(f'Number of flow feature vectors from globus-dtn1-src-iso2.csv: {len(globus_dtn1_src2)}')\n",
    "\n",
    "globus_dtn1_dest2 = data_dir + 'globus-dtn1-dest-iso2.csv'\n",
    "globus_dtn1_dest2, test_flows = extract_flow_features(globus_dtn1_dest2, kchow_ip)\n",
    "print(f'Number of flow feature vectors from globus-dtn1-dest-iso: {len(globus_dtn1_dest2)}')\n",
    "\n",
    "globus_clusterdtn_src = data_dir + 'globus-clusterdtn-src-iso.csv'\n",
    "globus_clusterdtn_src, test_flows = extract_flow_features(globus_clusterdtn_src, clustereddtn_ip)\n",
    "print(f'Number of flow feature vectors from globus-dtn1-src-iso2.csv: {len(globus_clusterdtn_src)}')\n",
    "\n",
    "globus_clusterdtn_dest = data_dir + 'globus-clusterdtn-dest-iso.csv'\n",
    "globus_clusterdtn_dest, test_flows = extract_flow_features(globus_clusterdtn_dest, kchow_ip)\n",
    "print(f'Number of flow feature vectors from globus-dtn1-dest-iso: {len(globus_clusterdtn_dest)}')\n",
    "\n",
    "# Verify the duration calculation\n",
    "for i, flow in enumerate(test_flows.keys()):\n",
    "#         print('start: ', test_flows[flow]['start'], '   end: ', test_flows[flow]['end'])\n",
    "        print(globus_clusterdtn_dest[i][25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flow feature vectors from fdt-airplane2-src-iso: 14\n",
      "Number of flow feature vectors from fdt-airplane2-dest-iso: 8\n",
      "Number of flow feature vectors from fdt-airplane2-dest-iso-1stream: 3\n",
      "Number of flow feature vectors from fdt-airplane2-dest-iso-2stream: 4\n",
      "Number of flow feature vectors from fdt-dtn1-dest-iso: 11\n",
      "Number of flow feature vectors from fdt-dtn1-src-iso: 11\n"
     ]
    }
   ],
   "source": [
    "# FDT Flows feature extractions\n",
    "fdt_a2_src = data_dir + 'fdt-airplane2-src-iso.csv'\n",
    "fdt_a2_src, test_flows = extract_flow_features(fdt_a2_src, airplane2_ip)\n",
    "print(f'Number of flow feature vectors from fdt-airplane2-src-iso: {len(fdt_a2_src)}')\n",
    "\n",
    "fdt_a2_dest = data_dir + 'fdt-airplane2-dest-iso.csv'\n",
    "fdt_a2_dest, test_flows = extract_flow_features(fdt_a2_dest, kchow_ip)\n",
    "print(f'Number of flow feature vectors from fdt-airplane2-dest-iso: {len(fdt_a2_dest)}')\n",
    "\n",
    "# 1 stream configured transfer\n",
    "fdt_a2_dest_1str = data_dir + 'fdt-airplane2-dest-iso-1stream.csv'\n",
    "fdt_a2_dest_1str, test_flows = extract_flow_features(fdt_a2_dest_1str, kchow_ip)\n",
    "print(f'Number of flow feature vectors from fdt-airplane2-dest-iso-1stream: {len(fdt_a2_dest_1str)}')\n",
    "\n",
    "# 2 stream configured transfer\n",
    "fdt_a2_dest_2str = data_dir + 'fdt-airplane2-dest-iso-2stream.csv'\n",
    "fdt_a2_dest_2str, test_flows = extract_flow_features(fdt_a2_dest_2str, kchow_ip)\n",
    "print(f'Number of flow feature vectors from fdt-airplane2-dest-iso-2stream: {len(fdt_a2_dest_2str)}')\n",
    "\n",
    "fdt_dtn1_dest = data_dir + 'fdt-dtn1-dest-iso.csv'\n",
    "fdt_dtn1_dest, test_flows = extract_flow_features(fdt_dtn1_dest, kchow_ip)\n",
    "print(f'Number of flow feature vectors from fdt-dtn1-dest-iso: {len(fdt_dtn1_dest)}')\n",
    "\n",
    "fdt_dtn1_src = data_dir + 'fdt-dtn1-src-iso.csv'\n",
    "fdt_dtn1_src, test_flows = extract_flow_features(fdt_dtn1_src, dtn1_ip)\n",
    "print(f'Number of flow feature vectors from fdt-dtn1-src-iso: {len(fdt_dtn1_src)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flow feature vectors from rclone-gdrive-src-iso: 5\n",
      "Number of flow feature vectors from rclone-gdrive-src-iso: 4\n",
      "Number of flow feature vectors from rclone-gdrive-src-iso2: 4\n",
      "Number of flow feature vectors from rclone-gdrive-dest-iso2: 4\n",
      "Number of flow feature vectors from rclone-gdrive-src-iso2: 4\n",
      "Number of flow feature vectors from rclone-gdrive-dest-iso3: 4\n"
     ]
    }
   ],
   "source": [
    "# RClone Flows feature extractions\n",
    "rclone_src = data_dir + 'rclone-gdrive-src-iso.csv'\n",
    "rclone_src, test_flows = extract_flow_features(rclone_src, gdrive_ip)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-src-iso: {len(rclone_src)}')\n",
    "\n",
    "rclone_dest = data_dir + 'rclone-gdrive-dest-iso.csv'\n",
    "rclone_dest, test_flows = extract_flow_features(rclone_dest, kchow_ip)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-src-iso: {len(rclone_dest)}')\n",
    "    \n",
    "rclone_src2 = data_dir + 'rclone-gdrive-src-iso2.csv'\n",
    "rclone_src2, test_flows = extract_flow_features(rclone_src2, gdrive_ip2)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-src-iso2: {len(rclone_src2)}')\n",
    "\n",
    "rclone_dest2 = data_dir + 'rclone-gdrive-dest-iso2.csv'\n",
    "rclone_dest2, test_flows = extract_flow_features(rclone_dest2, kchow_ip)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-dest-iso2: {len(rclone_dest2)}')\n",
    "\n",
    "rclone_src3 = data_dir + 'rclone-gdrive-src-iso3.csv'\n",
    "rclone_src3, test_flows = extract_flow_features(rclone_src3, gdrive_ip3)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-src-iso2: {len(rclone_src3)}')\n",
    "\n",
    "rclone_dest3 = data_dir + 'rclone-gdrive-dest-iso3.csv'\n",
    "rclone_dest3, test_flows = extract_flow_features(rclone_dest3, kchow_ip)\n",
    "print(f'Number of flow feature vectors from rclone-gdrive-dest-iso3: {len(rclone_dest3)}')\n",
    "\n",
    "# # Printing flows extracted\n",
    "# for key in test_flows.keys():\n",
    "#     print(key)\n",
    "    \n",
    "# # Sanity check on direction feature\n",
    "# for key in test_flows.keys():\n",
    "#     direction = test_flows[key]['direction']\n",
    "#     print(direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of all feature vectors mapping by capture file, saving it\n",
    "data_dict = {'globus_dtn1_src1' : globus_dtn1_src1, 'globus_dtn1_dest1' : globus_dtn1_dest1, \n",
    "             'globus_dtn1_src2' : globus_dtn1_src2, 'globus_dtn1_dest2' : globus_dtn1_dest2,\n",
    "             'globus_clusterdtn_src' : globus_clusterdtn_src, 'globus_clusterdtn_dest' : globus_clusterdtn_dest,\n",
    "             'fdt_a2_src' : fdt_a2_src, 'fdt_a2_dest' : fdt_a2_src, 'fdt_dtn1_dest' : fdt_dtn1_dest, \n",
    "             'fdt_dtn1_src' : fdt_dtn1_src, 'fdt_a2_dest_1str' : fdt_a2_dest_1str, 'fdt_a2_dest_2str' : fdt_a2_dest_2str,\n",
    "             'rclone_src' : rclone_src, 'rclone_dest' : rclone_dest, 'rclone_src2': rclone_src2, \n",
    "             'rclone_dest2': rclone_dest2, 'rclone_src3' : rclone_src3, 'rclone_dest3' : rclone_dest3}\n",
    "\n",
    "np.save('../Feature-Vectors/flow_features.npy', data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
